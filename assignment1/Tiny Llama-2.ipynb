{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qycf3ikvkUqP"
      },
      "source": [
        "## Assignment 1: Build a Toy Llama-2 Language Model\n",
        "\n",
        "> CISC7021 Applied Natural Language Processing (2024/2025)\n",
        "\n",
        "In this assignment, we will prepare a toy language model that employs the **Llama-2** architecture and evaluate the perplexity of the data set.\n",
        "\n",
        "We will learn how to perform continual pre-training of a base language model using the PyTorch and Hugging Face libraries. Detailed instructions for building this language model can be found in the attached notebook file.\n",
        "\n",
        "Acknowledgement: The base model checkpoint is converted from [llama2.c](https://github.com/karpathy/llama2.c) project. The data instances were sampled from [TinyStories](https://huggingface.co/datasets/roneneldan/TinyStories) dataset.\n",
        "\n",
        "---\n",
        "\n",
        "🚨 Please note that running this on CPU may be slow. If running on Google Colab or Kaggle, you can avoid this by going to **Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**. This should be included within the free tier of Colab.\n",
        "\n",
        "---\n",
        "\n",
        "We start by doing a `pip install` of all required libraries.\n",
        "- 🤗 `transformers`, `datasets`, `accelerate` are Huggingface libraries.\n",
        "- By default, Colab has `transformers`, `pytorch` libraries installed. If you are using a local machine, please install them via `pip` or `conda`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOhVvTEaa_b0"
      },
      "outputs": [],
      "source": [
        "#!pip install torch torchvision torchaudio\n",
        "#!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNC4vO-JkUqQ"
      },
      "outputs": [],
      "source": [
        "!pip install datasets accelerate -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WzqztoXkUqQ"
      },
      "source": [
        "### (Optional) Uploading the model/data to Google Colab or Kaggle.\n",
        "\n",
        "Please upload your dataset and model to computational platforms if you are using Colab or Kaggle environments.\n",
        "\n",
        "For Colab users, you can mount your Google Drive files by running the following code snippet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRSpL18W_Zfa",
        "outputId": "c489035e-c816-4552-cea9-133bc0492bf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqvZRLZYkUqR"
      },
      "source": [
        "### Necessary Packages, Environment Setups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Sa1iUH1ykUqR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "\n",
        "from typing import List, Optional, Tuple, Union\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer, AutoTokenizer\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from itertools import chain\n",
        "from datasets import load_dataset\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.nn import CrossEntropyLoss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBzFKWGZkUqR"
      },
      "source": [
        "Please set the correct file path based on your environment.\n",
        "\n",
        "- If you are using Colab, the path may be: `/content/drive/MyDrive/xxxxxx`\n",
        "- If you are using Kaggle, the path may be: `/kaggle/input/xxxxxx`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mYj0DvSgGmWq"
      },
      "outputs": [],
      "source": [
        "# Please set the correct file path based on your environment.\n",
        "TRAIN_FILE = 'data/zh_train.jsonl'\n",
        "VALIDATION_FILE = 'data/zh_dev.jsonl'\n",
        "TEST_FILE = 'data/zh_test.jsonl'\n",
        "EN_TEST_FILE = 'data/en_test.jsonl'\n",
        "MODEL_FOLDER = \"llama-42m\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElstHAMjkUqS"
      },
      "source": [
        "Load the model checkpoint into either a GPU or CPU (training will be slow on CPU, but decoding will be fair)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZF-0tQYDjvPl",
        "outputId": "70706b9e-30c1-470e-f8fc-5ba51c494bc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device type: cuda\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device type: {device}\")\n",
        "\n",
        "model_path = MODEL_FOLDER\n",
        "# Load model from local files\n",
        "model = LlamaForCausalLM.from_pretrained(model_path).to(device)\n",
        "# Load tokenizer from local files\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, device=device)\n",
        "\n",
        "if tokenizer.pad_token is None: # check if the tokenizer has defined pad_token (<pad>)\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g81Fac6kUqS"
      },
      "source": [
        "As we can see from the statistics, this model is much smaller than Llama-2 but shares the same decoder-only architecture.\n",
        "\n",
        "\n",
        "😄 **You do not need to check complex details!** We just present the architecture and number of parameters here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqxUbbA2u2uc",
        "outputId": "dd426913-1155-44dd-c932-b20683038c00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(32000, 512)\n",
            "    (layers): ModuleList(\n",
            "      (0-7): 8 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaSdpaAttention(\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=512, out_features=1376, bias=False)\n",
            "          (up_proj): Linear(in_features=512, out_features=1376, bias=False)\n",
            "          (down_proj): Linear(in_features=1376, out_features=512, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((512,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((512,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((512,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=512, out_features=32000, bias=False)\n",
            ")\n",
            "#Parameters: 41.69M\n"
          ]
        }
      ],
      "source": [
        "total_para = sum(v.numel() for k, v in model.state_dict().items() if k != 'model.embed_tokens.weight') / 1e6\n",
        "print(model)\n",
        "print(f\"#Parameters: {total_para:.2f}M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6LPYv5KkUqS"
      },
      "source": [
        "### Task 1: Decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxuzZuOrR4mE"
      },
      "source": [
        "\n",
        "If you are familar with the usage of `model.generate()` function in transformer library, please feel free to jump to [Task 1 Playground](#scrollTo=Task_1_Playground).\n",
        "\n",
        "\n",
        "#### 💡Tutorials: model.generate() function.\n",
        "---\n",
        "Minimal example:\n",
        "\n",
        "```python\n",
        "prompt = \"Once upon a time, \" # Input, prefix of generation\n",
        "```\n",
        "\n",
        "**Step 1**: Encode raw text using tokenizer model.\n",
        "```python\n",
        "tokenized_input = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "```\n",
        "\n",
        "**Step 2**: Set decoding hyper-parameters. Get the model output.\n",
        "```python\n",
        "output_ids = model.generate(tokenized_input, do_sample=True, max_new_tokens=300, temperature=0.6)\n",
        "```\n",
        "Important parameters:\n",
        "- `max_new_tokens`: The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
        "- `temperature`: The value of temperature used to modulate the next token probabilities. Higher temperature -> generate more diverse text. Lower temperature -> generate more deterministic text.\n",
        "- `do_sample`: `do_sample=False` is using greedy decoing strategy. To enable greedy decoding, we also need to set other sampling parameters `top_p`, `temperature` as `None`.\n",
        "- [If you are interested in other decoding algorithms, please refer to this link for setting parameters.](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/text_generation#transformers.GenerationConfig)\n",
        "\n",
        "**Step 3**: Convert model outputs into raw text.\n",
        "```python\n",
        "output_text = tokenizer.decode(output_ids[0])\n",
        "```\n",
        "or (when input instances >=1)\n",
        "```python\n",
        "output_text = tokenizer.batch_decode(output_ids)\n",
        "```\n",
        "Important parameters:\n",
        "- Setting `skip_special_tokens=True` will prevent special tokens, such as `<s>`, from appearing in the results..\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkvkY3LPkUqT"
      },
      "source": [
        "To understand the outputs of each step, let us do a simple generation task step by step! (Note: the base model is only able to produce fluent story text)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Gf6Q9qcgkUqT"
      },
      "outputs": [],
      "source": [
        "prompt = \"Once upon a time, Stella Lou had a dream.\" # Feel free to use other generation prefix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEKNYuWiJKIB",
        "outputId": "3c01550c-3b35-4c2d-c7b7-1fb44af22f5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[    1,  9038,  2501,   263,   931, 29892,   624,  3547,  4562,   750,\n",
            "           263, 12561, 29889]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Encode raw text using tokenizer model. Run tokenization and covert strings into token ids in vocabulary.\n",
        "tokenized_input = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "# See the tokenized results.\n",
        "print(tokenized_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlmCc71dElxz",
        "outputId": "5cf2d468-1269-492a-f72f-0a92742a139b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================Token IDs====================\n",
            "tensor([[    1,  9038,  2501,   263,   931, 29892,   624,  3547,  4562,   750,\n",
            "           263, 12561, 29889,  2296,  5131,   304,   367,   263, 12456,   985,\n",
            "         29889,  2296,  5131,   304, 19531,   263,  9560, 10714,   322,   263,\n",
            "           528,  4901, 20844, 29889,  1205,  1183,   471,  2086,  2319,   322,\n",
            "           278, 10714,   471,  2086,  4802, 29889,    13,  6716,  2462, 29892,\n",
            "           624,  3547,  4446,   263,  4802, 29892,   528,  4901, 10714,   297,\n",
            "           263,  3787, 29889,  2296,  4433,   902, 16823,   565,  1183,  1033,\n",
            "           505,   372, 29889,  2439, 16823,  1497,  4874,   322, 18093,   372,\n",
            "           363,   902, 29889,    13,   855,  3547,   471,   577,  9796, 29889,\n",
            "          2296,  1925,   373,   278, 10714,   322,  3252,   381,   839,  2820,\n",
            "         29889,  2296,  7091,   763,   263,  1855, 12456,   985, 29889,    13,\n",
            "          6246,   769, 29892,  1554,  8515,  9559, 29889,   624,  3547,  4687,\n",
            "           304,  4459,   270,   466,  1537, 29889,  2296,  8496, 29915, 29873,\n",
            "          2317,   701,  7812, 29889,  2296,  7091,   763,  1183,   471, 10917,\n",
            "          1076,  2820,   322,  2820, 29889,    13,   855,  3547, 29915, 29879,\n",
            "         16823,  4446,   902,   322,  1497, 29892,   376,   855,  3547, 29892,\n",
            "           366,   817,   304,  2125,   263,  2867, 29889,   887,  1106,   270,\n",
            "           466,  1537,  1213,    13,   855,  3547,  3614,  1283,   278, 10714,\n",
            "           322,  6568,  1623,   373,   278, 11904, 29889,  2296,  5764,   902,\n",
            "          5076,   322,  3614,   263,  6483, 16172, 29889,  2860,   263,  2846,\n",
            "          6233, 29892,  1183,  7091,  2253, 29889,    13,   855,  3547, 25156,\n",
            "           322,  1497, 29892,   376, 29924,   290, 29892,   306, 29915, 29885,\n",
            "          7960,   304,   367,   263, 12456,   985,  1449,  3850,     1]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Set decoding hyperparameters.\n",
        "\n",
        "# For greedy decoding\n",
        "max_new_tokens = 300\n",
        "do_sample = False  # `do_sample=False` means using greedy decoing strategy. To enable greedy decoding, we also need to set `top_p`, `temperature` as `None`.\n",
        "temperature = None\n",
        "\n",
        "# call generation function model.generate()\n",
        "output_ids = model.generate(\n",
        "    tokenized_input,\n",
        "    max_new_tokens=max_new_tokens,\n",
        "    eos_token_id=1,\n",
        "    do_sample=do_sample,\n",
        "    temperature=temperature,\n",
        "    top_p=None,\n",
        ")\n",
        "\n",
        "# The decoded results are token ids.\n",
        "print(\"=\" * 20 + \"Token IDs\" + \"=\" * 20)\n",
        "print(output_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQqRmKrXIYM1",
        "outputId": "124d1235-b532-4f7a-f51a-75f4ee36cb11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================Decoded Results====================\n",
            "Once upon a time, Stella Lou had a dream. She wanted to be a princess. She wanted to wear a beautiful dress and a shiny crown. But she was too small and the dress was too big.\n",
            "One day, Stella saw a big, shiny dress in a store. She asked her mom if she could have it. Her mom said yes and bought it for her.\n",
            "Stella was so happy. She put on the dress and twirled around. She felt like a real princess.\n",
            "But then, something strange happened. Stella started to feel dizzy. She couldn't stand up straight. She felt like she was spinning around and around.\n",
            "Stella's mom saw her and said, \"Stella, you need to take a break. You look dizzy.\"\n",
            "Stella took off the dress and lay down on the floor. She closed her eyes and took a deep breath. After a few minutes, she felt better.\n",
            "Stella smiled and said, \"Mom, I'm ready to be a princess again!\"\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Convert model outputs into raw text.\n",
        "# decode token ids into tokens\n",
        "print(\"=\" * 20 + \"Decoded Results\" + \"=\" * 20)\n",
        "# We only have one input instance. So we directly decode the first item of model output, i.e., `output_ids[0]`.\n",
        "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRNuqdJkRvxw"
      },
      "source": [
        "#### Another pipeline example: Sampling decoding with temperature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCqO5bkl341O",
        "outputId": "bad5340c-a6e7-4ce0-d4eb-389f0ea80c29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s> Once upon a time, Stella Lou had a dream. In her dream, she was in a big field. The field was full of flowers and trees. She was so happy!\n",
            "Suddenly, Stella heard a voice. It was a little girl. She said, \"Hi, I'm Mia. What's your name?\"\n",
            "Stella smiled and said, \"My name is Stella. I'm in this field.\"\n",
            "Mia said, \"It's so nice here. I'm so happy to be here.\"\n",
            "Stella said, \"Me too. I'm so glad you're here.\"\n",
            "Mia and Stella talked and laughed for a long time. They became good friends.\n",
            "When it was time to go, Stella said goodbye to Mia. She was so happy to have a new friend. She waved goodbye and went home.\n",
            "Stella's dream had come true! She was so happy.<s>\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Once upon a time, Stella Lou had a dream.\"\n",
        "\n",
        "# Decoding hyperparameters\n",
        "max_new_tokens = 300\n",
        "do_sample = True\n",
        "# The value of temperature used to modulate the next token probabilities.\n",
        "# Higher temperature -> generate more diverse text. Lower temperature -> generate more deterministic text.\n",
        "temperature = 0.3\n",
        "\n",
        "tokenized_input = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "output_ids = model.generate(\n",
        "    tokenized_input,\n",
        "    max_new_tokens=max_new_tokens,\n",
        "    eos_token_id=1,\n",
        "    do_sample=do_sample,\n",
        "    temperature=temperature,\n",
        ")\n",
        "output_text = tokenizer.decode(output_ids[0])\n",
        "print(output_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LT7or09aSBhp"
      },
      "source": [
        "#### Task 1 Playground\n",
        "\n",
        "---\n",
        "\n",
        "📚 Task 1: Please generate English stories using various prompts and decoding settings. Please feel free to explore any interesting phenomena, such as the impact of different prompts and the effects of various decoding algorithms and parameters. For example, quantify the text properties using linguistic-driven metrics like story length and Type-Token Ratio (TTR). In addition to objective metrics, you are encouraged to discuss your findings based on subjective case studies.\n",
        "\n",
        "We provide two types of skeleton code: one that takes a single prompt as input and another that can process batched inputs and decoding. Please use the version that best fits your preferences and data types.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 计算余弦相似度\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    vec1 = vec1.squeeze()\n",
        "    vec2 = vec2.squeeze()\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_a = np.linalg.norm(vec1)\n",
        "    norm_b = np.linalg.norm(vec2)\n",
        "    similarity = dot_product / (norm_a * norm_b)\n",
        "    return similarity\n",
        "\n",
        "# 获取句子的嵌入向量\n",
        "def get_sentence_vector(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        # 获取 token embeddings 并对其进行平均池化\n",
        "        token_embeddings = outputs.logits\n",
        "        sentence_vector = token_embeddings.mean(dim=1).cpu().numpy()\n",
        "    return sentence_vector.squeeze()\n",
        "\n",
        "# 绘制相似度矩阵\n",
        "def plot_similarity_matrix(response_list, dynamic_metrics, param_name):\n",
        "    similarity_matrix = np.zeros((len(response_list), len(response_list)))\n",
        "    vectors = [get_sentence_vector(response) for response in response_list]\n",
        "\n",
        "    # 计算余弦相似度\n",
        "    for i in range(len(response_list)):\n",
        "        for j in range(len(response_list)):\n",
        "            similarity_matrix[i][j] = cosine_similarity(vectors[i], vectors[j])\n",
        "    \n",
        "    fig, ax = plt.subplots()\n",
        "    cax = ax.matshow(similarity_matrix, cmap=plt.cm.Blues)\n",
        "    fig.colorbar(cax)\n",
        "    \n",
        "    # 设置动态参数组合的标签\n",
        "    ax.set_xticks(np.arange(len(response_list)))\n",
        "    ax.set_yticks(np.arange(len(response_list)))\n",
        "    ax.set_xticklabels([f'{param_name}={each:.2f}' for each in dynamic_metrics], rotation=45, ha=\"right\", fontsize=8)\n",
        "    ax.set_yticklabels([f'{param_name}={each:.2f}' for each in dynamic_metrics], fontsize=8)\n",
        "    \n",
        "    # 在矩阵格子中显示相似度值\n",
        "    for i in range(len(response_list)):\n",
        "        for j in range(len(response_list)):\n",
        "            similarity_value = f'{similarity_matrix[i][j]:.2f}'\n",
        "            ax.text(j, i, similarity_value, ha='center', va='center', color='black', fontsize=6)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"similarity_matrix_{param_name}.png\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "DNvZ5Q6rYeC4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s> Once upon a time, there was a big, big elephant. He was so large that he could not fit in the tiny house in the jungle. One day, he met a little monkey.\n",
            "\"Hello, little monkey. What are you doing?\" asked the elephant.\n",
            "\"I am looking for a place to play,\" said the monkey.\n",
            "The elephant said, \"I can help you. Follow me.\"\n",
            "The elephant led the monkey to a field where there were lots of trees and bushes. The monkey was happy and started to play.\n",
            "\"Thank you, Mr. Elephant. You are my friend,\" said the monkey.\n",
            "\"You are welcome, little monkey. I am happy to help,\" said the elephant.\n",
            "From that day on, the elephant and the monkey played together every day. The elephant would always lead the way and the monkey would follow him. They were the best of friends.<s>\n"
          ]
        }
      ],
      "source": [
        "# Skeleton Code: Single input (same as previous code blocks)\n",
        "\n",
        "prompt = \"\" # ⬅️ try to construct different prompts.\n",
        "\n",
        "# ⬇️ Try to tune different decoding hyperparameters.\n",
        "# You can also add more hyperparameters like `top_p`, `top_k`.\n",
        "max_new_tokens = 300\n",
        "do_sample = True\n",
        "temperature = 0.9\n",
        "\n",
        "tokenized_input = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "output_ids = model.generate(\n",
        "    tokenized_input,\n",
        "    max_new_tokens=max_new_tokens,\n",
        "    eos_token_id=1,\n",
        "    do_sample=do_sample,\n",
        "    temperature=temperature,\n",
        ")\n",
        "output_text = tokenizer.decode(output_ids[0])\n",
        "print(output_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80jDK-9uP1IO",
        "outputId": "ceed92d8-c965-4792-b121-ba5bbabab913"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Once upon a time,y was feeling very sad. He was feeling miserable because he had no friends to play with. He was all alone in his room, but then he saw something amazing - a big box! It was filled with lots of fun toys and games. He couldn't believe his eyes! He was so excited and happy, he couldn't help but smile.\n",
            "He opened the box and started playing with the toys. He was having so much fun that he didn't notice the door open. Suddenly, his mom walked in and saw the mess. She was very angry and shouted at him. She said, \"What have you done! You can't just leave the room like this! You have to clean up this mess!\"\n",
            "But the little boy didn't want to clean up the mess. He said, \"No, I don't want to! I just want to play!\" His mom was very angry and she said, \"If you don't clean up, I will punish you!\"\n",
            "The little boy didn't want to get punished, so he reluctantly started cleaning. He worked very hard and soon the mess was gone. His mom was so happy and said, \"Good job! Now you can go back to playing with your toys.\" The little boy smiled and went back to playing with his toys. He was so happy that he had found a way to make his mom happy.\n",
            "\n",
            "Tom is a cute kitty. He likes to play with his ball and his mouse. He likes to chase mice and birds and bugs. He likes to sleep on his soft bed and dream of new things.\n",
            "One day, Tom sees a big box in the living room. He is curious. He goes to the box and sniffs it. He hears a noise inside. He scratches the box and hears a growl. He is scared. He runs away from the box.\n",
            "Mom and Dad hear the noise. They come to the living room. They see the box and the box. They open the box. They see Tom and the box. They are not happy. They scold Tom. They tell him to go to his room. They tell him he can not play with the box anymore.\n",
            "Tom is sad. He goes to his room. He cries. He hugs his mouse. He wishes he had not opened the box. He wishes he had not been scared. He wishes he had a happy ending.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Skeleton Code: Bacthed input-output\n",
        "\n",
        "prompts = [\"Once upon a time,\", \"Tom is a cute kitty.\"]  # ⬅️ try to construct different prompts.\n",
        "# prompts = [\"Once upon a time, there was a\", \"Jianpeng Zhao is a Ph.D student at the University of Macau.\", \"When I was a child,\", \"This story is\"]\n",
        "\n",
        "batch_size = 5 # If you have multiple data inputs, please control the batch size to prevent out-of-memory issues.\n",
        "\n",
        "# ⬇️ Try to tune different decoding hyperparameters.\n",
        "# You can also add more hyperparameters like `top_p`, `top_k`.\n",
        "max_new_tokens = 300\n",
        "do_sample = True\n",
        "temperature = 0.6\n",
        "top_p = 0.9\n",
        "top_k = 5\n",
        "\n",
        "for i in range(0, len(prompts), batch_size):\n",
        "    batch_input = prompts[i:i+batch_size]\n",
        "    tokenized_input = tokenizer(batch_input, return_tensors=\"pt\", padding=True).to(device)\n",
        "\n",
        "    # For decoder-only models, batched inputs of model.generate() should be in the format of input_ids.\n",
        "    output_ids = model.generate(\n",
        "        tokenized_input[\"input_ids\"],\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        eos_token_id=1,\n",
        "        do_sample=do_sample,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    output_text = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
        "\n",
        "    for idx, result in enumerate(output_text):\n",
        "        print(f\"{result}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhfIINEMSIjp"
      },
      "source": [
        "#### What about other languages?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYlvKScxkUqT"
      },
      "source": [
        "Oops! This English language model cannot generate stories in other languages!\n",
        "\n",
        "Why? Let us evaluate the perplexity of different languages in the next task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0jb_ZqW_ixo",
        "outputId": "e5f718d0-8025-438e-e5d5-79eb2ce7932c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s> 从前有一只小兔子乖乖ons were playing in the park. He was running around, laughing and having a great time. Suddenly, he saw a big, red ball. He ran over to it and started to play with it. \n",
            "\"Hey!\" shouted a little girl. \"That's my ball!\"\n",
            "\"No!\" yelled a little girl. \"That's my ball!\"\n",
            "The little girl started to cry. \"Please give it back,\" she said.\n",
            "The little boy didn't want to give it back. He wanted to keep playing with it. He started to cry too. \n",
            "The little girl's mom came over. She saw the little girl crying and the little boy crying. \n",
            "\"What's wrong?\" she asked.\n",
            "The little girl said, \"He took my ball and won't give it back!\"\n",
            "The mom smiled. \"It's ok,\" she said. \"Let's go get your ball.\"\n",
            "The little girl smiled and they all went to get her ball. The little boy was happy to have his ball back and he was very happy to play with it again.<s>\n"
          ]
        }
      ],
      "source": [
        "prompt = \"从前有一只小兔子乖乖\"\n",
        "\n",
        "# Decoding hyperparameters\n",
        "max_new_tokens = 300\n",
        "do_sample = True\n",
        "temperature = 0.3\n",
        "\n",
        "tokenized_input = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "output_ids = model.generate(\n",
        "    tokenized_input,\n",
        "    max_new_tokens=max_new_tokens,\n",
        "    eos_token_id=1,\n",
        "    do_sample=do_sample,\n",
        "    temperature=temperature,\n",
        ")\n",
        "output_text = tokenizer.decode(output_ids[0])\n",
        "print(output_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYst_QWTkUqU"
      },
      "source": [
        "### Task 2: Perplexity Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6NxwPeaBzOB"
      },
      "source": [
        "#### Background\n",
        "\n",
        "---\n",
        "\n",
        "The perplexity serves as a key metric for evaluating language models. It quantifies how well a model predicts a sample, with lower perplexity indicating better performance. For a tokenized sequence $X = (x_0, x_1, \\dots, x_t)$, the perplexity is defined mathematically as:\n",
        "\n",
        "$$\\text{Perplexity}(X) = \\exp \\left( -\\frac{1}{t} \\sum_{i=1}^t \\log p_\\theta (x_i | x_{<i}) \\right)$$\n",
        "\n",
        "Here, $p_\\theta(x_i | x_{<i})$ represents the probability of a token $ x_i $ given its preceding tokens, and the formulation incorporates the average log probability across the sequence.\n",
        "\n",
        "---\n",
        "\n",
        "⚠️ Please make sure to **run the following cell first** to define the evaluation function.\n",
        "\n",
        "😄 **You do not need to check these complex details! Too hard for beginners!** However, if you are interested, you can compare the following code with the explanations above to better understand how to implement PPL evaluation using PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "bEIdmBRsJaP9"
      },
      "outputs": [],
      "source": [
        "# The following code was adapted from the `evaluate` library. Licensed under the Apache License, Version 2.0 (the \"License\").\n",
        "# We modify them to avoid causing serious memory issues in the Colab environment.\n",
        "\n",
        "def compute_ppl(\n",
        "        model, tokenizer, inputs, device, batch_size: int = 16, add_start_token: bool = True, max_length=None\n",
        "):\n",
        "\n",
        "    if device is not None:\n",
        "        assert device in [\"gpu\", \"cpu\", \"cuda\"], \"device should be either gpu or cpu.\"\n",
        "        if device == \"gpu\":\n",
        "            device = \"cuda\"\n",
        "    else:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # if batch_size > 1 (which generally leads to padding being required), and\n",
        "    # if there is not an already assigned pad_token, assign an existing\n",
        "    # special token to also be the padding token\n",
        "    if tokenizer.pad_token is None and batch_size > 1:\n",
        "        existing_special_tokens = list(tokenizer.special_tokens_map_extended.values())\n",
        "        # check that the model already has at least one special token defined\n",
        "        assert (\n",
        "            len(existing_special_tokens) > 0\n",
        "        ), \"If batch_size > 1, model must have at least one special token to use for padding. Please use a different model or set batch_size=1.\"\n",
        "        # assign one of the special tokens to also be the pad token\n",
        "        tokenizer.add_special_tokens({\"pad_token\": existing_special_tokens[0]})\n",
        "\n",
        "    if add_start_token and max_length:\n",
        "        # leave room for <BOS> token to be added:\n",
        "        assert (\n",
        "            tokenizer.bos_token is not None\n",
        "        ), \"Input model must already have a BOS token if using add_start_token=True. Please use a different model, or set add_start_token=False\"\n",
        "        max_tokenized_len = max_length - 1\n",
        "    else:\n",
        "        max_tokenized_len = max_length\n",
        "\n",
        "    encodings = tokenizer(\n",
        "        inputs,\n",
        "        add_special_tokens=False,\n",
        "        padding=True,\n",
        "        truncation=True if max_tokenized_len else False,\n",
        "        max_length=max_tokenized_len,\n",
        "        return_tensors=\"pt\",\n",
        "        return_attention_mask=True,\n",
        "    )\n",
        "\n",
        "    encoded_texts = encodings[\"input_ids\"]\n",
        "    attn_masks = encodings[\"attention_mask\"]\n",
        "\n",
        "    # check that each input is long enough:\n",
        "    if add_start_token:\n",
        "        assert torch.all(torch.ge(attn_masks.sum(1), 1)), \"Each input text must be at least one token long.\"\n",
        "    else:\n",
        "        assert torch.all(\n",
        "            torch.ge(attn_masks.sum(1), 2)\n",
        "        ), \"When add_start_token=False, each input text must be at least two tokens long. Run with add_start_token=True if inputting strings of only one token, and remove all empty input strings.\"\n",
        "\n",
        "    ppls = []\n",
        "    loss_fct = CrossEntropyLoss(reduction=\"none\")\n",
        "\n",
        "    for start_index in tqdm(range(0, len(encoded_texts), batch_size)):\n",
        "        end_index = min(start_index + batch_size, len(encoded_texts))\n",
        "        encoded_batch = encoded_texts[start_index:end_index].to(device)\n",
        "        attn_mask = attn_masks[start_index:end_index].to(device)\n",
        "\n",
        "        if add_start_token:\n",
        "            bos_tokens_tensor = torch.tensor([[tokenizer.bos_token_id]] * encoded_batch.size(dim=0)).to(device)\n",
        "            encoded_batch = torch.cat([bos_tokens_tensor, encoded_batch], dim=1)\n",
        "            attn_mask = torch.cat(\n",
        "                [torch.ones(bos_tokens_tensor.size(), dtype=torch.int64).to(device), attn_mask], dim=1\n",
        "            )\n",
        "\n",
        "        labels = encoded_batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out_logits = model(encoded_batch, attention_mask=attn_mask).logits\n",
        "\n",
        "            shift_logits = out_logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "            shift_attention_mask_batch = attn_mask[..., 1:].contiguous()\n",
        "\n",
        "            perplexity_batch = torch.exp(\n",
        "                (loss_fct(shift_logits.transpose(1, 2), shift_labels) * shift_attention_mask_batch).sum(1)\n",
        "                / shift_attention_mask_batch.sum(1)\n",
        "            )\n",
        "\n",
        "            ppls += perplexity_batch.tolist()\n",
        "\n",
        "    del encoded_batch, attn_mask\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return {\"perplexities\": ppls, \"mean_perplexity\": sum(ppls)/float(len(ppls))}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TscJWpxB2XH"
      },
      "source": [
        "\n",
        "#### 💡Tutorials: compute_ppl() function.\n",
        "\n",
        "---\n",
        "Minimal example:\n",
        "\n",
        "```python\n",
        "test_dataset = [\"Once upon a time,\"]\n",
        "\n",
        "compute_ppl(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device,\n",
        "    inputs=test_dataset,\n",
        "    batch_size = 16\n",
        ")\n",
        "```\n",
        "\n",
        "Important parameters:\n",
        "- `inputs`: list of input text, each separate text snippet is one list entry.\n",
        "- `batch_size`: the batch size to run evaluations.\n",
        "\n",
        "Returns:\n",
        "- `perplexity`: `{\"perplexities\": [x.x, x.x, ...], \"mean_perplexity\": x.x}` dictionary containing the perplexity scores for the texts in the input list, as well as the mean perplexity. .\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yDr5pIVCHCP"
      },
      "source": [
        "#### Task 2 Playground\n",
        "\n",
        "---\n",
        "\n",
        "📚 Task 2: Evaluate the perplexity. Ensure that you evaluate both the English and Chinese test data we provided. You are encouraged to collect more diverse text data and discuss your findings regarding the language understanding capacity of the base model.\n",
        "\n",
        "\n",
        "Note: If you want to reuse the evaluation codes for JSONL data, please structure the content as follows:\n",
        "```json\n",
        "{\"text\": \"one data\"}\n",
        "{\"text\": \"two data.\"}\n",
        "...\n",
        "```\n",
        "**You may find that the PPL value for Chinese text is significantly higher than that for English text. This is evidence that the base model cannot generate a Chinese story at the end of the last task.**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "35d813e7b32f44e49d38155e4f950c86",
            "4245c28c8eec4edd921b1755ad1ad45b",
            "2c286ad6343a475d8516ac4779b208ac",
            "62beee732f344d1fabf7e213930ad775",
            "037842bf542143038269aedb14c51c0d",
            "7f004def454c46f4a489d83a265577f6",
            "5407caa8f2df48179033c1d0d6daebfe",
            "fa04910012354222b3f1a49ed5e33203",
            "b45c1921cebe42418c9bac889be111de",
            "645cdc1efc3344dc8625f9ed98ee4433",
            "b2f56d482fdd43dea48c9f649680e11b"
          ]
        },
        "id": "QSWMQtwDSdIm",
        "outputId": "0841a9fc-fe40-499b-aae7-15d202149077"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c03e5042a6a459a89a82eb05625be3f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexity: 10.68\n"
          ]
        }
      ],
      "source": [
        "# Skeleton Code: Evaluate the perplexity (PPL) on a list of raw text.\n",
        "\n",
        "test_dataset = [\"Once upon a time,\", \"Tom is a cute kitty.\"] # ⬅️ you can use your examples / or read from raw text file\n",
        "\n",
        "results = compute_ppl(model=model, tokenizer=tokenizer, device=device, inputs=test_dataset, batch_size = 16)\n",
        "dataset_ppl = results['mean_perplexity']\n",
        "print(f\"Perplexity: {dataset_ppl:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "2df92583f44640f38112db08ea30cc5e",
            "9ef56642df394267850e820ae578d3d7",
            "fb11f7b6a113420cbf9353c79f434ad5",
            "b039f8470ffa4535ad065fc0eda886fd",
            "d2f6deec117a4358ad064db89f727bc1",
            "459a84f78164433884ba5f470a9e1417",
            "19620102f85d4c6a8b48e1078a180e95",
            "8995660b5da643a495cc5d8d23104913",
            "2ae1f05679b84ac9bac39c77485e6b01",
            "e063f6ac5f4a44b88271e700e985185a",
            "325cd0c8a932466d9c115ffba2904bba",
            "1a50911f63024bf59da9873dd6d1a912",
            "e9b6ee204f4a457ab9863c0f3baf4864",
            "40a0c17e7fde4bf2862ecda678d5e963",
            "e0cdab58ab0c4936878db9d524c8726c",
            "b2143d465f7047629cdfe9777e3a286b",
            "e56cd9f3c4124896a1f04d55b19888ea",
            "67c58390dc914bb6a8db182ab1943157",
            "76f64cb2e08e453d977b20150745fe41",
            "35fe5e0e1ce14594b643f9679ea2b7f3",
            "b5769bc0ec3e4662b08fc27d8dda40c8",
            "91f68453eca74af8804b1871a20a0b26"
          ]
        },
        "id": "4a_qx_9LLHYI",
        "outputId": "a66756f4-6ae8-4d8b-e1dd-7697d6e75966"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "702763d28c294c438ddeeea6cf764aa1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/63 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(English Text) Test Perplexity: 4.14\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09fbdbd19d7b4954993f5f6e5a45f1e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/63 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(Chinese Text) Test Perplexity: 70030.34\n"
          ]
        }
      ],
      "source": [
        "# Skeleton Code: Evaluate the perplexity (PPL) on an external test set file (JSONL).\n",
        "\n",
        "# English test set.\n",
        "data_file = EN_TEST_FILE # ⬅️ you can change your file path\n",
        "test_dataset = load_dataset('json', data_files={'test': data_file})[\"test\"][\"text\"]\n",
        "\n",
        "results = compute_ppl(model=model, tokenizer=tokenizer, device=device, inputs=test_dataset, batch_size = 16)\n",
        "dataset_ppl = results['mean_perplexity']\n",
        "print(f\"(English Text) Test Perplexity: {dataset_ppl:.2f}\")\n",
        "\n",
        "# Chinese test set.\n",
        "data_file = TEST_FILE # ⬅️ you can change your file path\n",
        "test_dataset = load_dataset('json', data_files={'test': data_file})[\"test\"][\"text\"]\n",
        "\n",
        "results = compute_ppl(model=model, tokenizer=tokenizer, device=device, inputs=test_dataset, batch_size = 16)\n",
        "dataset_ppl = results['mean_perplexity']\n",
        "print(f\"(Chinese Text) Test Perplexity: {dataset_ppl:.2f}\")\n",
        "\n",
        "\n",
        "# Try your own data file!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "MVELmXzJP7GS"
      },
      "outputs": [],
      "source": [
        "# 🚨 Release gpu cache before training the model\n",
        "if device == \"cuda\":\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gilWue9kUqU"
      },
      "source": [
        "### Task 3: Continual Pre-training (in Chinese or in another language you are proficient in)\n",
        "\n",
        "Currently, our base English LM is proficient in English but lacks the capability to generate or comprehend other languages (e.g., Chinese). The objective of this task is to enhance a base English LM by continually pre-training it with text in another language. This process aims to enable the model to understand and generate mini-story in another language.\n",
        "\n",
        "We have provided 10,000 Chinese training samples. The training process for any language is the same. We have included useful resource links (in Assignment description PDF) to help you create additional data. If you encounter any issues in creating a dataset in another language, please do not hesitate to contact us.\n",
        "\n",
        "We have implemented data preprocessing and the training pipeline, so you are not required to optimize these components. Instead, focus on tuning the training hyperparameters and observe the changes in model performance.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "⚠️ Please **make sure to run the following cell first to pre-process data**.\n",
        "\n",
        "😄 You do not need to check the details of whole pipeline construction! Please pay attention to the hyper-parameters of `trainer`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzYCSeGVkUqT"
      },
      "source": [
        "#### Preprocess Data\n",
        "Here, we preprocess (tokenize and group) the text for the subsequent evaluation and pre-training phases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhQN9DpQkUqT"
      },
      "source": [
        "Load prepared Chinese dataset from Google drive (or local disk)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "SiGEfoUMkUqT",
        "outputId": "66d7b579-4295-41e9-9b75-60e2c941d378"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 10000\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 500\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 1000\n",
            "    })\n",
            "})\n",
            "从前，有一个小女孩名叫莉莉。她喜欢和家人一起去度假。有一天，她的家人决定去海边旅行。莉莉非常兴奋，她跳起来又跳下去，像发了疯一样。\n",
            "\n",
            "当他们到达海滩时，他们搭起了遮阳伞和毯子。莉莉想立刻去游泳，但她的父母告诉她要等吃完午餐再说。莉莉感到很不耐烦，她说：“我现在就想去游泳！”她妈妈回答：“莉莉，我们需要先吃东西。游泳需要能量。”\n",
            "\n",
            "莉莉意识到妈妈说得对，于是耐心地等待午餐结束。她学会了有时候要克制自己的激动情绪，并听从父母的意见。从那天起，莉莉变得更善于倾听，也更加享受她的假期时光。\n"
          ]
        }
      ],
      "source": [
        "chinese_dataset = load_dataset('json', data_files={'train': TRAIN_FILE, 'validation':VALIDATION_FILE, 'test': TEST_FILE})\n",
        "print(chinese_dataset)\n",
        "print(chinese_dataset[\"test\"][2][\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAcSghJgkUqT"
      },
      "source": [
        "We tokenize the raw text using Llama-2's tokenizer and group the tokenized text as inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "tUTCHIqEWJRL"
      },
      "outputs": [],
      "source": [
        "block_size = 512\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"])\n",
        "\n",
        "def group_texts(examples):\n",
        "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    if total_length >= block_size:\n",
        "        total_length = (total_length // block_size) * block_size\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "dXoXO29hWY-v"
      },
      "outputs": [],
      "source": [
        "tokenized_zh_datasets = chinese_dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])\n",
        "lm_datasets = tokenized_zh_datasets.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    batch_size=512,\n",
        "    num_proc=4,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_m505-vnSBg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DUTC5J7nSZt"
      },
      "source": [
        "#### 💡Tutorials: TrainingArguments().\n",
        "\n",
        "**Important Training Hyper-parameters**\n",
        "- learning_rate: The initial learning rate for optimizer.\n",
        "- num_train_epochs: Total number of training epochs to perform (if not an integer, will perform the decimal part percents of the last epoch before stopping training).\n",
        "- *_strategy: The evaluation/saving strategy to adopt during training. Possible values are:\n",
        "    - `\"no\"`: No evaluation/saving is done during training.\n",
        "    - `\"steps\"`: Evaluation/saving is done (and logged) every `eval_steps`.\n",
        "    - `\"epoch\"`: Evaluation/saving is done at the end of each epoch.\n",
        "- per_device_train_batch_size: The batch size per GPU/XPU/TPU/MPS/NPU core/CPU for training.\n",
        "- per_device_eval_batch_size: The batch size per GPU/XPU/TPU/MPS/NPU core/CPU for evaluation.\n",
        "- save_total_limit: If a value is passed, will limit the total amount of checkpoints.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "If you do not understand `AdamW` optimizer and learning scheduler, you may use default settings.\n",
        "\n",
        "**Optimizer Hyper-parameters**\n",
        "- weight_decay: The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in [`AdamW`] optimizer.\n",
        "- adam_beta1: The beta1 hyperparameter for the [`AdamW`] optimizer.\n",
        "- adam_beta2: The beta2 hyperparameter for the [`AdamW`] optimizer.\n",
        "\n",
        "**Learning schedule**\n",
        "- lr_scheduler: The scheduler type to use.\n",
        "- warmup_ratio: Ratio of total training steps used for a linear warmup from 0 to `learning_rate`.\n",
        "\n",
        "[Explore more parameters here](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/trainer#transformers.TrainingArguments)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9OJF3VNhrGK"
      },
      "source": [
        "#### Task 3 Playground\n",
        "\n",
        "---\n",
        "\n",
        "📚 Please just run the following code to do continual pre-training. Please try your best to tune the hyperparameters or collect more data to improve model performance.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Lf6bS7lnbPMM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jpzhao/miniconda3/envs/CISC7021/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# =========Pre-training hyperparameters, please feel free to tune them~=========\n",
        "# =Important=\n",
        "lr = 1e-4\n",
        "epochs = 8\n",
        "save_steps=200\n",
        "strategy=\"steps\"\n",
        "train_bsz = 32 # reduce batch size if you encountered out-of-memory errors.\n",
        "eval_bsz = 16\n",
        "\n",
        "# If you do not understand AdamW optimizer and learning scheduler, you may use default settings.\n",
        "# =Optimizer=\n",
        "optimizer = \"adamw_torch\"\n",
        "weight_decay = 0.01\n",
        "adam_beta1 = 0.9\n",
        "adam_beta2 = 0.98\n",
        "# =Learning scheduler=\n",
        "lr_scheduler = \"linear\"\n",
        "warmup_ratio = 0.01\n",
        "# =========End of pre-training hyperparameters=========\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    \"llama-42m-zh-fairytales\",\n",
        "    evaluation_strategy = strategy,\n",
        "    eval_steps=save_steps,\n",
        "    save_strategy = strategy,\n",
        "    save_steps=save_steps,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps = 10,\n",
        "    learning_rate=lr,\n",
        "    weight_decay=weight_decay,\n",
        "    seed=42,\n",
        "    per_device_train_batch_size=train_bsz,\n",
        "    per_device_eval_batch_size=eval_bsz,\n",
        "    save_total_limit=1,\n",
        "    optim = optimizer,\n",
        "    lr_scheduler_type = lr_scheduler,\n",
        "    adam_beta1 = adam_beta1,\n",
        "    adam_beta2 = adam_beta2,\n",
        "    warmup_ratio = warmup_ratio,\n",
        "    num_train_epochs = epochs,\n",
        "    report_to=None\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=lm_datasets[\"train\"],\n",
        "    eval_dataset=lm_datasets[\"validation\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "KCIR43fZcAa9",
        "outputId": "adecb9fd-1375-42d9-f7b2-60b91e4dc1de"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2000/2000 13:17, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>3.321500</td>\n",
              "      <td>3.274351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.897700</td>\n",
              "      <td>1.890903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.542000</td>\n",
              "      <td>1.566165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.376700</td>\n",
              "      <td>1.435025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.307900</td>\n",
              "      <td>1.370766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>1.246400</td>\n",
              "      <td>1.322082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>1.173700</td>\n",
              "      <td>1.296775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>1.123300</td>\n",
              "      <td>1.278089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>1.081700</td>\n",
              "      <td>1.270050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>1.097800</td>\n",
              "      <td>1.264119</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2000, training_loss=1.7560472617149352, metrics={'train_runtime': 798.1341, 'train_samples_per_second': 79.876, 'train_steps_per_second': 2.506, 'total_flos': 4956004181606400.0, 'train_loss': 1.7560472617149352, 'epoch': 8.0})"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjA9aBL4kUqV"
      },
      "source": [
        "Load pre-trained model and try to generate mini-story in another language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "lE_QVLCOkUqV",
        "outputId": "cf3ddb1a-5e89-473d-eeb8-7e32d87b7cb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device type: cuda\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"Device type: {device}\")\n",
        "\n",
        "new_model_path = \"llama-42m-zh-fairytales/checkpoint-2000\" # saved checkpoint path\n",
        "model = LlamaForCausalLM.from_pretrained(new_model_path).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, device=device)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlX2XilNkUqV"
      },
      "source": [
        "Evaluate the PPL on Chinese text (or another language) again.\n",
        "\n",
        "You will notice that we actually achieve a much lower PPL after continual pre-training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "0Ob_g328cGq5",
        "outputId": "e7f2b619-f3c8-4dc2-bbc2-76fac657b2b6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0d3a46ec94354e84bc5715d1b20304ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/63 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Perplexity: 3.46\n"
          ]
        }
      ],
      "source": [
        "data_file = TEST_FILE\n",
        "test_dataset = load_dataset('json', data_files={'test': data_file})[\"test\"][\"text\"]\n",
        "\n",
        "results = compute_ppl(model=model, tokenizer=tokenizer, device=device, inputs=test_dataset, batch_size = 16)\n",
        "dataset_ppl = results['mean_perplexity']\n",
        "print(f\"Test Perplexity: {dataset_ppl:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xgi1EMDycZUp"
      },
      "source": [
        "---\n",
        "\n",
        "The original English base model was pre-trained on 2 million data samples. Considering we are using only 10,000 training samples (0.5% of the original pre-training data), the model can generate a few fluent sentences but may still struggle with long-text generation or common sense of other languages. You can try using more data or training steps depending on your computational resources.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Rf9ALF4CEX82",
        "outputId": "66d35223-1871-4d92-a008-cd50f257db4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s> 从前，有一只叫做汤姆的猫。汤姆和他的朋友们一起玩。他们喜欢在外面玩耍和奔跑。\n",
            "\n",
            "有一天，汤姆和他的朋友们在玩球时，发现了一个大箱子。他们想知道里面有什么。他们想要一个办法让他们感到快乐。\n",
            "\n",
            "汤姆和他的朋友们决定玩一个游戏。他们把球扔得高高的，然后扔进了箱子里。他们把球扔得高高的，然后把它们扔进了箱子里。汤姆和他的朋友们很高兴，他们整天都在玩球。<s>\n"
          ]
        }
      ],
      "source": [
        "prompt = \"从前，有一只叫做汤姆的猫。汤姆和他的朋友们一起玩。\"\n",
        "\n",
        "# Decoding hyperparameters\n",
        "max_new_tokens = 300\n",
        "do_sample = True\n",
        "temperature = 0.3\n",
        "\n",
        "tokenized_input = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "output_ids = model.generate(\n",
        "    tokenized_input,\n",
        "    max_new_tokens=max_new_tokens,\n",
        "    eos_token_id=1,\n",
        "    do_sample=do_sample,\n",
        "    temperature=temperature,\n",
        ")\n",
        "output_text = tokenizer.decode(output_ids[0])\n",
        "print(output_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJYZCXX3kUqZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "037842bf542143038269aedb14c51c0d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19620102f85d4c6a8b48e1078a180e95": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a50911f63024bf59da9873dd6d1a912": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e9b6ee204f4a457ab9863c0f3baf4864",
              "IPY_MODEL_40a0c17e7fde4bf2862ecda678d5e963",
              "IPY_MODEL_e0cdab58ab0c4936878db9d524c8726c"
            ],
            "layout": "IPY_MODEL_b2143d465f7047629cdfe9777e3a286b"
          }
        },
        "2ae1f05679b84ac9bac39c77485e6b01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2c286ad6343a475d8516ac4779b208ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa04910012354222b3f1a49ed5e33203",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b45c1921cebe42418c9bac889be111de",
            "value": 1
          }
        },
        "2df92583f44640f38112db08ea30cc5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9ef56642df394267850e820ae578d3d7",
              "IPY_MODEL_fb11f7b6a113420cbf9353c79f434ad5",
              "IPY_MODEL_b039f8470ffa4535ad065fc0eda886fd"
            ],
            "layout": "IPY_MODEL_d2f6deec117a4358ad064db89f727bc1"
          }
        },
        "325cd0c8a932466d9c115ffba2904bba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35d813e7b32f44e49d38155e4f950c86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4245c28c8eec4edd921b1755ad1ad45b",
              "IPY_MODEL_2c286ad6343a475d8516ac4779b208ac",
              "IPY_MODEL_62beee732f344d1fabf7e213930ad775"
            ],
            "layout": "IPY_MODEL_037842bf542143038269aedb14c51c0d"
          }
        },
        "35fe5e0e1ce14594b643f9679ea2b7f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "40a0c17e7fde4bf2862ecda678d5e963": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76f64cb2e08e453d977b20150745fe41",
            "max": 63,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_35fe5e0e1ce14594b643f9679ea2b7f3",
            "value": 63
          }
        },
        "4245c28c8eec4edd921b1755ad1ad45b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f004def454c46f4a489d83a265577f6",
            "placeholder": "​",
            "style": "IPY_MODEL_5407caa8f2df48179033c1d0d6daebfe",
            "value": "100%"
          }
        },
        "459a84f78164433884ba5f470a9e1417": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5407caa8f2df48179033c1d0d6daebfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62beee732f344d1fabf7e213930ad775": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_645cdc1efc3344dc8625f9ed98ee4433",
            "placeholder": "​",
            "style": "IPY_MODEL_b2f56d482fdd43dea48c9f649680e11b",
            "value": " 1/1 [00:00&lt;00:00, 19.33it/s]"
          }
        },
        "645cdc1efc3344dc8625f9ed98ee4433": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67c58390dc914bb6a8db182ab1943157": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76f64cb2e08e453d977b20150745fe41": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f004def454c46f4a489d83a265577f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8995660b5da643a495cc5d8d23104913": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91f68453eca74af8804b1871a20a0b26": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ef56642df394267850e820ae578d3d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_459a84f78164433884ba5f470a9e1417",
            "placeholder": "​",
            "style": "IPY_MODEL_19620102f85d4c6a8b48e1078a180e95",
            "value": "100%"
          }
        },
        "b039f8470ffa4535ad065fc0eda886fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e063f6ac5f4a44b88271e700e985185a",
            "placeholder": "​",
            "style": "IPY_MODEL_325cd0c8a932466d9c115ffba2904bba",
            "value": " 63/63 [00:48&lt;00:00,  1.54it/s]"
          }
        },
        "b2143d465f7047629cdfe9777e3a286b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2f56d482fdd43dea48c9f649680e11b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b45c1921cebe42418c9bac889be111de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b5769bc0ec3e4662b08fc27d8dda40c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2f6deec117a4358ad064db89f727bc1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e063f6ac5f4a44b88271e700e985185a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0cdab58ab0c4936878db9d524c8726c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5769bc0ec3e4662b08fc27d8dda40c8",
            "placeholder": "​",
            "style": "IPY_MODEL_91f68453eca74af8804b1871a20a0b26",
            "value": " 63/63 [01:18&lt;00:00,  1.07s/it]"
          }
        },
        "e56cd9f3c4124896a1f04d55b19888ea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9b6ee204f4a457ab9863c0f3baf4864": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e56cd9f3c4124896a1f04d55b19888ea",
            "placeholder": "​",
            "style": "IPY_MODEL_67c58390dc914bb6a8db182ab1943157",
            "value": "100%"
          }
        },
        "fa04910012354222b3f1a49ed5e33203": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb11f7b6a113420cbf9353c79f434ad5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8995660b5da643a495cc5d8d23104913",
            "max": 63,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2ae1f05679b84ac9bac39c77485e6b01",
            "value": 63
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
