{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qycf3ikvkUqP"
      },
      "source": [
        "## Assignment 1: Build a Toy Llama-2 Language Model\n",
        "\n",
        "> CISC7021 Applied Natural Language Processing (2024/2025)\n",
        "\n",
        "In this assignment, we will prepare a toy language model that employs the **Llama-2** architecture and evaluate the perplexity of the data set.\n",
        "\n",
        "We will learn how to perform continual pre-training of a base language model using the PyTorch and Hugging Face libraries. Detailed instructions for building this language model can be found in the attached notebook file.\n",
        "\n",
        "Acknowledgement: The base model checkpoint is converted from [llama2.c](https://github.com/karpathy/llama2.c) project. The data instances were sampled from [TinyStories](https://huggingface.co/datasets/roneneldan/TinyStories) dataset.\n",
        "\n",
        "---\n",
        "\n",
        "🚨 Please note that running this on CPU may be slow. If running on Google Colab or Kaggle, you can avoid this by going to **Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**. This should be included within the free tier of Colab.\n",
        "\n",
        "---\n",
        "\n",
        "We start by doing a `pip install` of all required libraries.\n",
        "- 🤗 `transformers`, `datasets`, `accelerate` are Huggingface libraries.\n",
        "- By default, Colab has `transformers`, `pytorch` libraries installed. If you are using a local machine, please install them via `pip` or `conda`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOhVvTEaa_b0"
      },
      "outputs": [],
      "source": [
        "#!pip install torch torchvision torchaudio\n",
        "#!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNC4vO-JkUqQ"
      },
      "outputs": [],
      "source": [
        "!pip install datasets accelerate -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WzqztoXkUqQ"
      },
      "source": [
        "### (Optional) Uploading the model/data to Google Colab or Kaggle.\n",
        "\n",
        "Please upload your dataset and model to computational platforms if you are using Colab or Kaggle environments.\n",
        "\n",
        "For Colab users, you can mount your Google Drive files by running the following code snippet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRSpL18W_Zfa",
        "outputId": "c489035e-c816-4552-cea9-133bc0492bf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqvZRLZYkUqR"
      },
      "source": [
        "### Necessary Packages, Environment Setups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Sa1iUH1ykUqR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "\n",
        "from typing import List, Optional, Tuple, Union\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer, AutoTokenizer\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from itertools import chain\n",
        "from datasets import load_dataset\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.nn import CrossEntropyLoss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBzFKWGZkUqR"
      },
      "source": [
        "Please set the correct file path based on your environment.\n",
        "\n",
        "- If you are using Colab, the path may be: `/content/drive/MyDrive/xxxxxx`\n",
        "- If you are using Kaggle, the path may be: `/kaggle/input/xxxxxx`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mYj0DvSgGmWq"
      },
      "outputs": [],
      "source": [
        "# Please set the correct file path based on your environment.\n",
        "TRAIN_FILE = 'data/zh_train.jsonl'\n",
        "VALIDATION_FILE = 'data/zh_dev.jsonl'\n",
        "TEST_FILE = 'data/zh_test.jsonl'\n",
        "EN_TEST_FILE = 'data/en_test.jsonl'\n",
        "MODEL_FOLDER = \"llama-42m\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElstHAMjkUqS"
      },
      "source": [
        "Load the model checkpoint into either a GPU or CPU (training will be slow on CPU, but decoding will be fair)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZF-0tQYDjvPl",
        "outputId": "70706b9e-30c1-470e-f8fc-5ba51c494bc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device type: cuda\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device type: {device}\")\n",
        "\n",
        "model_path = MODEL_FOLDER\n",
        "# Load model from local files\n",
        "model = LlamaForCausalLM.from_pretrained(model_path).to(device)\n",
        "# Load tokenizer from local files\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, device=device)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g81Fac6kUqS"
      },
      "source": [
        "As we can see from the statistics, this model is much smaller than Llama-2 but shares the same decoder-only architecture.\n",
        "\n",
        "\n",
        "😄 **You do not need to check complex details!** We just present the architecture and number of parameters here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqxUbbA2u2uc",
        "outputId": "dd426913-1155-44dd-c932-b20683038c00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(32000, 512)\n",
            "    (layers): ModuleList(\n",
            "      (0-7): 8 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaSdpaAttention(\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=512, out_features=1376, bias=False)\n",
            "          (up_proj): Linear(in_features=512, out_features=1376, bias=False)\n",
            "          (down_proj): Linear(in_features=1376, out_features=512, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm()\n",
            "        (post_attention_layernorm): LlamaRMSNorm()\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=512, out_features=32000, bias=False)\n",
            ")\n",
            "#Parameters: 41.69M\n"
          ]
        }
      ],
      "source": [
        "total_para = sum(v.numel() for k, v in model.state_dict().items() if k != 'model.embed_tokens.weight') / 1e6\n",
        "print(model)\n",
        "print(f\"#Parameters: {total_para:.2f}M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6LPYv5KkUqS"
      },
      "source": [
        "### Task 1: Decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxuzZuOrR4mE"
      },
      "source": [
        "\n",
        "If you are familar with the usage of `model.generate()` function in transformer library, please feel free to jump to [Task 1 Playground](#scrollTo=Task_1_Playground).\n",
        "\n",
        "\n",
        "#### 💡Tutorials: model.generate() function.\n",
        "---\n",
        "Minimal example:\n",
        "\n",
        "```python\n",
        "prompt = \"Once upon a time, \" # Input, prefix of generation\n",
        "```\n",
        "\n",
        "**Step 1**: Encode raw text using tokenizer model.\n",
        "```python\n",
        "tokenized_input = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "```\n",
        "\n",
        "**Step 2**: Set decoding hyper-parameters. Get the model output.\n",
        "```python\n",
        "output_ids = model.generate(tokenized_input, do_sample=True, max_new_tokens=300, temperature=0.6)\n",
        "```\n",
        "Important parameters:\n",
        "- `max_new_tokens`: The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
        "- `temperature`: The value of temperature used to modulate the next token probabilities. Higher temperature -> generate more diverse text. Lower temperature -> generate more deterministic text.\n",
        "- `do_sample`: `do_sample=False` is using greedy decoing strategy. To enable greedy decoding, we also need to set other sampling parameters `top_p`, `temperature` as `None`.\n",
        "- [If you are interested in other decoding algorithms, please refer to this link for setting parameters.](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/text_generation#transformers.GenerationConfig)\n",
        "\n",
        "**Step 3**: Convert model outputs into raw text.\n",
        "```python\n",
        "output_text = tokenizer.decode(output_ids[0])\n",
        "```\n",
        "or (when input instances >=1)\n",
        "```python\n",
        "output_text = tokenizer.batch_decode(output_ids)\n",
        "```\n",
        "Important parameters:\n",
        "- Setting `skip_special_tokens=True` will prevent special tokens, such as `<s>`, from appearing in the results..\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkvkY3LPkUqT"
      },
      "source": [
        "To understand the outputs of each step, let us do a simple generation task step by step! (Note: the base model is only able to produce fluent story text)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gf6Q9qcgkUqT"
      },
      "outputs": [],
      "source": [
        "prompt = \"Once upon a time, Stella Lou had a dream.\" # Feel free to use other generation prefix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEKNYuWiJKIB",
        "outputId": "3c01550c-3b35-4c2d-c7b7-1fb44af22f5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[    1,  9038,  2501,   263,   931, 29892,   624,  3547,  4562,   750,\n",
            "           263, 12561, 29889]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Encode raw text using tokenizer model. Run tokenization and covert strings into token ids in vocabulary.\n",
        "tokenized_input = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "# See the tokenized results.\n",
        "print(tokenized_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlmCc71dElxz",
        "outputId": "5cf2d468-1269-492a-f72f-0a92742a139b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================Token IDs====================\n",
            "tensor([[    1,  9038,  2501,   263,   931, 29892,   624,  3547,  4562,   750,\n",
            "           263, 12561, 29889,  2296,  5131,   304,   367,   263, 12456,   985,\n",
            "         29889,  2296,  5131,   304, 19531,   263,  9560, 10714,   322,   263,\n",
            "           528,  4901, 20844, 29889,  1205,  1183,   471,  2086,  2319,   322,\n",
            "           278, 10714,   471,  2086,  4802, 29889,    13,  6716,  2462, 29892,\n",
            "           624,  3547,  4446,   263,  4802, 29892,   528,  4901, 10714,   297,\n",
            "           263,  3787, 29889,  2296,  4433,   902, 16823,   565,  1183,  1033,\n",
            "           505,   372, 29889,  2439, 16823,  1497,  4874,   322, 18093,   372,\n",
            "           363,   902, 29889,    13,   855,  3547,   471,   577,  9796, 29889,\n",
            "          2296,  1925,   373,   278, 10714,   322,  3252,   381,   839,  2820,\n",
            "         29889,  2296,  7091,   763,   263,  1855, 12456,   985, 29889,    13,\n",
            "          6246,   769, 29892,  1554,  8515,  9559, 29889,   624,  3547,  4687,\n",
            "           304,  4459,   270,   466,  1537, 29889,  2296,  8496, 29915, 29873,\n",
            "          2317,   701,  7812, 29889,  2296,  7091,   763,  1183,   471, 10917,\n",
            "          1076,  2820,   322,  2820, 29889,    13,   855,  3547, 29915, 29879,\n",
            "         16823,  4446,   902,   322,  1497, 29892,   376,   855,  3547, 29892,\n",
            "           366,   817,   304,  2125,   263,  2867, 29889,   887,  1106,   270,\n",
            "           466,  1537,  1213,    13,   855,  3547,  3614,  1283,   278, 10714,\n",
            "           322,  6568,  1623,   373,   278, 11904, 29889,  2296,  5764,   902,\n",
            "          5076,   322,  3614,   263,  6483, 16172, 29889,  2860,   263,  2846,\n",
            "          6233, 29892,  1183,  7091,  2253, 29889,    13,   855,  3547, 25156,\n",
            "           322,  1497, 29892,   376, 29924,   290, 29892,   306, 29915, 29885,\n",
            "          7960,   304,   367,   263, 12456,   985,  1449,  3850,     1]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Set decoding hyperparameters.\n",
        "\n",
        "# For greedy decoding\n",
        "max_new_tokens = 300\n",
        "do_sample = False  # `do_sample=False` means using greedy decoing strategy. To enable greedy decoding, we also need to set `top_p`, `temperature` as `None`.\n",
        "temperature = None\n",
        "\n",
        "# call generation function model.generate()\n",
        "output_ids = model.generate(\n",
        "    tokenized_input,\n",
        "    max_new_tokens=max_new_tokens,\n",
        "    eos_token_id=1,\n",
        "    do_sample=do_sample,\n",
        "    temperature=temperature,\n",
        "    top_p=None,\n",
        ")\n",
        "\n",
        "# The decoded results are token ids.\n",
        "print(\"=\" * 20 + \"Token IDs\" + \"=\" * 20)\n",
        "print(output_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQqRmKrXIYM1",
        "outputId": "124d1235-b532-4f7a-f51a-75f4ee36cb11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================Decoded Results====================\n",
            "Once upon a time, Stella Lou had a dream. She wanted to be a princess. She wanted to wear a beautiful dress and a shiny crown. But she was too small and the dress was too big.\n",
            "One day, Stella saw a big, shiny dress in a store. She asked her mom if she could have it. Her mom said yes and bought it for her.\n",
            "Stella was so happy. She put on the dress and twirled around. She felt like a real princess.\n",
            "But then, something strange happened. Stella started to feel dizzy. She couldn't stand up straight. She felt like she was spinning around and around.\n",
            "Stella's mom saw her and said, \"Stella, you need to take a break. You look dizzy.\"\n",
            "Stella took off the dress and lay down on the floor. She closed her eyes and took a deep breath. After a few minutes, she felt better.\n",
            "Stella smiled and said, \"Mom, I'm ready to be a princess again!\"\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Convert model outputs into raw text.\n",
        "# decode token ids into tokens\n",
        "print(\"=\" * 20 + \"Decoded Results\" + \"=\" * 20)\n",
        "# We only have one input instance. So we directly decode the first item of model output, i.e., `output_ids[0]`.\n",
        "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRNuqdJkRvxw"
      },
      "source": [
        "#### Another pipeline example: Sampling decoding with temperature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCqO5bkl341O",
        "outputId": "bad5340c-a6e7-4ce0-d4eb-389f0ea80c29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s> Once upon a time, Stella Lou had a dream. She wanted to be the most popular girl in the world. Everywhere she went, people would smile and say how much they liked her.\n",
            "One day, Stella was walking down the street when she saw a little girl. The girl was wearing a pretty dress and had a big smile on her face. Stella was so excited that she ran up to the girl and said, \"Hi! I'm Stella Lou. What's your name?\"\n",
            "The little girl smiled and said, \"My name is Sarah. I'm so happy to meet you!\"\n",
            "Stella Lou was so happy that she started to dance around Sarah. She said, \"Let's go to the park and play together!\"\n",
            "So, Stella and Sarah went to the park and played all day. They laughed and had so much fun. Stella Lou was so happy that she had made a new friend.\n",
            "At the end of the day, Stella Lou said goodbye to Sarah and thanked her for being her friend. She went home and fell asleep with a big smile on her face. She dreamt of all the fun she had with Sarah the next day.<s>\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Once upon a time, Stella Lou had a dream.\"\n",
        "\n",
        "# Decoding hyperparameters\n",
        "max_new_tokens = 300\n",
        "do_sample = True\n",
        "# The value of temperature used to modulate the next token probabilities.\n",
        "# Higher temperature -> generate more diverse text. Lower temperature -> generate more deterministic text.\n",
        "temperature = 0.3\n",
        "\n",
        "tokenized_input = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "output_ids = model.generate(\n",
        "    tokenized_input,\n",
        "    max_new_tokens=max_new_tokens,\n",
        "    eos_token_id=1,\n",
        "    do_sample=do_sample,\n",
        "    temperature=temperature,\n",
        ")\n",
        "output_text = tokenizer.decode(output_ids[0])\n",
        "print(output_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LT7or09aSBhp"
      },
      "source": [
        "#### Task 1 Playground\n",
        "\n",
        "---\n",
        "\n",
        "📚 Task 1: Please generate English stories using various prompts and decoding settings. Please feel free to explore any interesting phenomena, such as the impact of different prompts and the effects of various decoding algorithms and parameters. For example, quantify the text properties using linguistic-driven metrics like story length and Type-Token Ratio (TTR). In addition to objective metrics, you are encouraged to discuss your findings based on subjective case studies.\n",
        "\n",
        "We provide two types of skeleton code: one that takes a single prompt as input and another that can process batched inputs and decoding. Please use the version that best fits your preferences and data types.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNvZ5Q6rYeC4"
      },
      "outputs": [],
      "source": [
        "# Skeleton Code: Single input (same as previous code blocks)\n",
        "\n",
        "prompt = \"\" # ⬅️ try to construct different prompts.\n",
        "\n",
        "# ⬇️ Try to tune different decoding hyperparameters.\n",
        "# You can also add more hyperparameters like `top_p`, `top_k`.\n",
        "max_new_tokens = 300\n",
        "do_sample = True\n",
        "temperature = 0.3\n",
        "\n",
        "tokenized_input = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "output_ids = model.generate(\n",
        "    tokenized_input,\n",
        "    max_new_tokens=max_new_tokens,\n",
        "    eos_token_id=1,\n",
        "    do_sample=do_sample,\n",
        "    temperature=temperature,\n",
        ")\n",
        "output_text = tokenizer.decode(output_ids[0])\n",
        "print(output_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80jDK-9uP1IO",
        "outputId": "ceed92d8-c965-4792-b121-ba5bbabab913"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Once upon a time, and her mommy went to the park. They saw a big puddle and mommy said, \"Let's splash in it!\" But the puddle was too deep and mommy said, \"No, we can't. We will get all wet!\" \n",
            "Suddenly, a big, scary dog came running towards them. Mommy said, \"Oh no! That dog is dangerous!\" But the dog just wanted to play and jumped into the puddle. The dog splashed and splashed until mommy was all wet. \n",
            "Mommy laughed and said, \"That was fun! Let's go home now.\" And they walked away, leaving the dangerous dog behind.\n",
            "\n",
            "Tom is a cute kitty. He likes to play with his ball and his mouse. He also likes to eat fish and chicken. But he does not like to sleep in his crib. His crib is big and soft and has a soft blanket.\n",
            "One night, Tom's mom comes to his crib. She says, \"Tom, it is time to sleep. You need to rest your eyes and your ears. Sleeping is good for you. It helps you grow and learn and be happy.\"\n",
            "Tom does not want to sleep in his crib. He wants to play with his ball and his mouse. He says, \"No, mom, I do not want to sleep. I want to play. Sleeping is boring.\"\n",
            "Tom's mom says, \"Tom, you have to sleep in your crib. It is safe and cozy and has your toys. You can play with your ball and your mouse later. Sleeping is important. It makes you strong and smart and brave. It also helps you dream and dream and dream. You can dream of anything you want.\"\n",
            "Tom thinks for a moment. He looks at his ball and his mouse. He looks at his crib. He looks at his mom. He says, \"Okay, mom, I will sleep in my crib. But can I have a hug and a kiss?\"\n",
            "Tom's mom smiles and hugs him. She says,\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Skeleton Code: Bacthed input-output\n",
        "\n",
        "prompts = [\"Once upon a time,\", \"Tom is a cute kitty.\"]  # ⬅️ try to construct different prompts.\n",
        "\n",
        "batch_size = 2 # If you have multiple data inputs, please control the batch size to prevent out-of-memory issues.\n",
        "\n",
        "# ⬇️ Try to tune different decoding hyperparameters.\n",
        "# You can also add more hyperparameters like `top_p`, `top_k`.\n",
        "max_new_tokens = 300\n",
        "do_sample = True\n",
        "temperature = 0.6\n",
        "\n",
        "for i in range(0, len(prompts), batch_size):\n",
        "    batch_input = prompts[i:i+batch_size]\n",
        "    tokenized_input = tokenizer(batch_input, return_tensors=\"pt\", padding=True).to(device)\n",
        "\n",
        "    # For decoder-only models, batched inputs of model.generate() should be in the format of input_ids.\n",
        "    output_ids = model.generate(\n",
        "        tokenized_input[\"input_ids\"],\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        eos_token_id=1,\n",
        "        do_sample=do_sample,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    output_text = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
        "\n",
        "    for idx, result in enumerate(output_text):\n",
        "        print(f\"{result}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhfIINEMSIjp"
      },
      "source": [
        "#### What about other languages?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYlvKScxkUqT"
      },
      "source": [
        "Oops! This English language model cannot generate stories in other languages!\n",
        "\n",
        "Why? Let us evaluate the perplexity of different languages in the next task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0jb_ZqW_ixo",
        "outputId": "e5f718d0-8025-438e-e5d5-79eb2ce7932c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s> 从前有一只小兔子乖乖ons. They were very excited to go to the park.\n",
            "When they got there, they saw a big, red slide. They ran over to it and started to slide down. They laughed and giggled as they slid down.\n",
            "When they got to the bottom, they saw a big, blue ball. They both wanted to play with it.\n",
            "\"Let's play with the ball,\" said Sammy.\n",
            "\"No, let's play with the ball,\" said Mommy.\n",
            "They argued for a while, but then Mommy had an idea.\n",
            "\"Why don't we take turns? You can play with the ball first, and then Sammy can play with it.\"\n",
            "Sammy and Mommy agreed. Sammy played with the ball first, then Mommy played with the ball. Sammy had lots of fun.\n",
            "When it was Sammy's turn, he was so excited. He ran over to the ball and started to play with it. Mommy watched him and smiled.\n",
            "\"That's a good idea Sammy,\" said Mommy. \"It's important to share and take turns.\"\n",
            "Sammy nodded and smiled. He was happy that Mommy was so understanding.<s>\n"
          ]
        }
      ],
      "source": [
        "prompt = \"从前有一只小兔子乖乖\"\n",
        "\n",
        "# Decoding hyperparameters\n",
        "max_new_tokens = 300\n",
        "do_sample = True\n",
        "temperature = 0.3\n",
        "\n",
        "tokenized_input = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "output_ids = model.generate(\n",
        "    tokenized_input,\n",
        "    max_new_tokens=max_new_tokens,\n",
        "    eos_token_id=1,\n",
        "    do_sample=do_sample,\n",
        "    temperature=temperature,\n",
        ")\n",
        "output_text = tokenizer.decode(output_ids[0])\n",
        "print(output_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYst_QWTkUqU"
      },
      "source": [
        "### Task 2: Perplexity Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6NxwPeaBzOB"
      },
      "source": [
        "#### Background\n",
        "\n",
        "---\n",
        "\n",
        "The perplexity serves as a key metric for evaluating language models. It quantifies how well a model predicts a sample, with lower perplexity indicating better performance. For a tokenized sequence $X = (x_0, x_1, \\dots, x_t)$, the perplexity is defined mathematically as:\n",
        "\n",
        "$$\\text{Perplexity}(X) = \\exp \\left( -\\frac{1}{t} \\sum_{i=1}^t \\log p_\\theta (x_i | x_{<i}) \\right)$$\n",
        "\n",
        "Here, $p_\\theta(x_i | x_{<i})$ represents the probability of a token $ x_i $ given its preceding tokens, and the formulation incorporates the average log probability across the sequence.\n",
        "\n",
        "---\n",
        "\n",
        "⚠️ Please make sure to **run the following cell first** to define the evaluation function.\n",
        "\n",
        "😄 **You do not need to check these complex details! Too hard for beginners!** However, if you are interested, you can compare the following code with the explanations above to better understand how to implement PPL evaluation using PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEIdmBRsJaP9"
      },
      "outputs": [],
      "source": [
        "# The following code was adapted from the `evaluate` library. Licensed under the Apache License, Version 2.0 (the \"License\").\n",
        "# We modify them to avoid causing serious memory issues in the Colab environment.\n",
        "\n",
        "def compute_ppl(\n",
        "        model, tokenizer, inputs, device, batch_size: int = 16, add_start_token: bool = True, max_length=None\n",
        "):\n",
        "\n",
        "    if device is not None:\n",
        "        assert device in [\"gpu\", \"cpu\", \"cuda\"], \"device should be either gpu or cpu.\"\n",
        "        if device == \"gpu\":\n",
        "            device = \"cuda\"\n",
        "    else:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # if batch_size > 1 (which generally leads to padding being required), and\n",
        "    # if there is not an already assigned pad_token, assign an existing\n",
        "    # special token to also be the padding token\n",
        "    if tokenizer.pad_token is None and batch_size > 1:\n",
        "        existing_special_tokens = list(tokenizer.special_tokens_map_extended.values())\n",
        "        # check that the model already has at least one special token defined\n",
        "        assert (\n",
        "            len(existing_special_tokens) > 0\n",
        "        ), \"If batch_size > 1, model must have at least one special token to use for padding. Please use a different model or set batch_size=1.\"\n",
        "        # assign one of the special tokens to also be the pad token\n",
        "        tokenizer.add_special_tokens({\"pad_token\": existing_special_tokens[0]})\n",
        "\n",
        "    if add_start_token and max_length:\n",
        "        # leave room for <BOS> token to be added:\n",
        "        assert (\n",
        "            tokenizer.bos_token is not None\n",
        "        ), \"Input model must already have a BOS token if using add_start_token=True. Please use a different model, or set add_start_token=False\"\n",
        "        max_tokenized_len = max_length - 1\n",
        "    else:\n",
        "        max_tokenized_len = max_length\n",
        "\n",
        "    encodings = tokenizer(\n",
        "        inputs,\n",
        "        add_special_tokens=False,\n",
        "        padding=True,\n",
        "        truncation=True if max_tokenized_len else False,\n",
        "        max_length=max_tokenized_len,\n",
        "        return_tensors=\"pt\",\n",
        "        return_attention_mask=True,\n",
        "    )\n",
        "\n",
        "    encoded_texts = encodings[\"input_ids\"]\n",
        "    attn_masks = encodings[\"attention_mask\"]\n",
        "\n",
        "    # check that each input is long enough:\n",
        "    if add_start_token:\n",
        "        assert torch.all(torch.ge(attn_masks.sum(1), 1)), \"Each input text must be at least one token long.\"\n",
        "    else:\n",
        "        assert torch.all(\n",
        "            torch.ge(attn_masks.sum(1), 2)\n",
        "        ), \"When add_start_token=False, each input text must be at least two tokens long. Run with add_start_token=True if inputting strings of only one token, and remove all empty input strings.\"\n",
        "\n",
        "    ppls = []\n",
        "    loss_fct = CrossEntropyLoss(reduction=\"none\")\n",
        "\n",
        "    for start_index in tqdm(range(0, len(encoded_texts), batch_size)):\n",
        "        end_index = min(start_index + batch_size, len(encoded_texts))\n",
        "        encoded_batch = encoded_texts[start_index:end_index].to(device)\n",
        "        attn_mask = attn_masks[start_index:end_index].to(device)\n",
        "\n",
        "        if add_start_token:\n",
        "            bos_tokens_tensor = torch.tensor([[tokenizer.bos_token_id]] * encoded_batch.size(dim=0)).to(device)\n",
        "            encoded_batch = torch.cat([bos_tokens_tensor, encoded_batch], dim=1)\n",
        "            attn_mask = torch.cat(\n",
        "                [torch.ones(bos_tokens_tensor.size(), dtype=torch.int64).to(device), attn_mask], dim=1\n",
        "            )\n",
        "\n",
        "        labels = encoded_batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out_logits = model(encoded_batch, attention_mask=attn_mask).logits\n",
        "\n",
        "            shift_logits = out_logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "            shift_attention_mask_batch = attn_mask[..., 1:].contiguous()\n",
        "\n",
        "            perplexity_batch = torch.exp(\n",
        "                (loss_fct(shift_logits.transpose(1, 2), shift_labels) * shift_attention_mask_batch).sum(1)\n",
        "                / shift_attention_mask_batch.sum(1)\n",
        "            )\n",
        "\n",
        "            ppls += perplexity_batch.tolist()\n",
        "\n",
        "    del encoded_batch, attn_mask\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return {\"perplexities\": ppls, \"mean_perplexity\": sum(ppls)/float(len(ppls))}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TscJWpxB2XH"
      },
      "source": [
        "\n",
        "#### 💡Tutorials: compute_ppl() function.\n",
        "\n",
        "---\n",
        "Minimal example:\n",
        "\n",
        "```python\n",
        "test_dataset = [\"Once upon a time,\"]\n",
        "\n",
        "compute_ppl(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device,\n",
        "    inputs=test_dataset,\n",
        "    batch_size = 16\n",
        ")\n",
        "```\n",
        "\n",
        "Important parameters:\n",
        "- `inputs`: list of input text, each separate text snippet is one list entry.\n",
        "- `batch_size`: the batch size to run evaluations.\n",
        "\n",
        "Returns:\n",
        "- `perplexity`: `{\"perplexities\": [x.x, x.x, ...], \"mean_perplexity\": x.x}` dictionary containing the perplexity scores for the texts in the input list, as well as the mean perplexity. .\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yDr5pIVCHCP"
      },
      "source": [
        "#### Task 2 Playground\n",
        "\n",
        "---\n",
        "\n",
        "📚 Task 2: Evaluate the perplexity. Ensure that you evaluate both the English and Chinese test data we provided. You are encouraged to collect more diverse text data and discuss your findings regarding the language understanding capacity of the base model.\n",
        "\n",
        "\n",
        "Note: If you want to reuse the evaluation codes for JSONL data, please structure the content as follows:\n",
        "```json\n",
        "{\"text\": \"one data\"}\n",
        "{\"text\": \"two data.\"}\n",
        "...\n",
        "```\n",
        "**You may find that the PPL value for Chinese text is significantly higher than that for English text. This is evidence that the base model cannot generate a Chinese story at the end of the last task.**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "35d813e7b32f44e49d38155e4f950c86",
            "4245c28c8eec4edd921b1755ad1ad45b",
            "2c286ad6343a475d8516ac4779b208ac",
            "62beee732f344d1fabf7e213930ad775",
            "037842bf542143038269aedb14c51c0d",
            "7f004def454c46f4a489d83a265577f6",
            "5407caa8f2df48179033c1d0d6daebfe",
            "fa04910012354222b3f1a49ed5e33203",
            "b45c1921cebe42418c9bac889be111de",
            "645cdc1efc3344dc8625f9ed98ee4433",
            "b2f56d482fdd43dea48c9f649680e11b"
          ]
        },
        "id": "QSWMQtwDSdIm",
        "outputId": "0841a9fc-fe40-499b-aae7-15d202149077"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "35d813e7b32f44e49d38155e4f950c86",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexity: 10.68\n"
          ]
        }
      ],
      "source": [
        "# Skeleton Code: Evaluate the perplexity (PPL) on a list of raw text.\n",
        "\n",
        "test_dataset = [\"Once upon a time,\", \"Tom is a cute kitty.\"] # ⬅️ you can use your examples / or read from raw text file\n",
        "\n",
        "results = compute_ppl(model=model, tokenizer=tokenizer, device=device, inputs=test_dataset, batch_size = 16)\n",
        "dataset_ppl = results['mean_perplexity']\n",
        "print(f\"Perplexity: {dataset_ppl:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "2df92583f44640f38112db08ea30cc5e",
            "9ef56642df394267850e820ae578d3d7",
            "fb11f7b6a113420cbf9353c79f434ad5",
            "b039f8470ffa4535ad065fc0eda886fd",
            "d2f6deec117a4358ad064db89f727bc1",
            "459a84f78164433884ba5f470a9e1417",
            "19620102f85d4c6a8b48e1078a180e95",
            "8995660b5da643a495cc5d8d23104913",
            "2ae1f05679b84ac9bac39c77485e6b01",
            "e063f6ac5f4a44b88271e700e985185a",
            "325cd0c8a932466d9c115ffba2904bba",
            "1a50911f63024bf59da9873dd6d1a912",
            "e9b6ee204f4a457ab9863c0f3baf4864",
            "40a0c17e7fde4bf2862ecda678d5e963",
            "e0cdab58ab0c4936878db9d524c8726c",
            "b2143d465f7047629cdfe9777e3a286b",
            "e56cd9f3c4124896a1f04d55b19888ea",
            "67c58390dc914bb6a8db182ab1943157",
            "76f64cb2e08e453d977b20150745fe41",
            "35fe5e0e1ce14594b643f9679ea2b7f3",
            "b5769bc0ec3e4662b08fc27d8dda40c8",
            "91f68453eca74af8804b1871a20a0b26"
          ]
        },
        "id": "4a_qx_9LLHYI",
        "outputId": "a66756f4-6ae8-4d8b-e1dd-7697d6e75966"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2df92583f44640f38112db08ea30cc5e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/63 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(English Text) Test Perplexity: 4.14\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a50911f63024bf59da9873dd6d1a912",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/63 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(Chinese Text) Test Perplexity: 70030.42\n"
          ]
        }
      ],
      "source": [
        "# Skeleton Code: Evaluate the perplexity (PPL) on an external test set file (JSONL).\n",
        "\n",
        "# English test set.\n",
        "data_file = EN_TEST_FILE # ⬅️ you can change your file path\n",
        "test_dataset = load_dataset('json', data_files={'test': data_file})[\"test\"][\"text\"]\n",
        "\n",
        "results = compute_ppl(model=model, tokenizer=tokenizer, device=device, inputs=test_dataset, batch_size = 16)\n",
        "dataset_ppl = results['mean_perplexity']\n",
        "print(f\"(English Text) Test Perplexity: {dataset_ppl:.2f}\")\n",
        "\n",
        "# Chinese test set.\n",
        "data_file = TEST_FILE # ⬅️ you can change your file path\n",
        "test_dataset = load_dataset('json', data_files={'test': data_file})[\"test\"][\"text\"]\n",
        "\n",
        "results = compute_ppl(model=model, tokenizer=tokenizer, device=device, inputs=test_dataset, batch_size = 16)\n",
        "dataset_ppl = results['mean_perplexity']\n",
        "print(f\"(Chinese Text) Test Perplexity: {dataset_ppl:.2f}\")\n",
        "\n",
        "\n",
        "# Try your own data file!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVELmXzJP7GS"
      },
      "outputs": [],
      "source": [
        "# 🚨 Release gpu cache before training the model\n",
        "if device == \"cuda\":\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gilWue9kUqU"
      },
      "source": [
        "### Task 3: Continual Pre-training (in Chinese or in another language you are proficient in)\n",
        "\n",
        "Currently, our base English LM is proficient in English but lacks the capability to generate or comprehend other languages (e.g., Chinese). The objective of this task is to enhance a base English LM by continually pre-training it with text in another language. This process aims to enable the model to understand and generate mini-story in another language.\n",
        "\n",
        "We have provided 10,000 Chinese training samples. The training process for any language is the same. We have included useful resource links (in Assignment description PDF) to help you create additional data. If you encounter any issues in creating a dataset in another language, please do not hesitate to contact us.\n",
        "\n",
        "We have implemented data preprocessing and the training pipeline, so you are not required to optimize these components. Instead, focus on tuning the training hyperparameters and observe the changes in model performance.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "⚠️ Please **make sure to run the following cell first to pre-process data**.\n",
        "\n",
        "😄 You do not need to check the details of whole pipeline construction! Please pay attention to the hyper-parameters of `trainer`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzYCSeGVkUqT"
      },
      "source": [
        "#### Preprocess Data\n",
        "Here, we preprocess (tokenize and group) the text for the subsequent evaluation and pre-training phases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhQN9DpQkUqT"
      },
      "source": [
        "Load prepared Chinese dataset from Google drive (or local disk)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiGEfoUMkUqT",
        "outputId": "66d7b579-4295-41e9-9b75-60e2c941d378"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 10000\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 500\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 1000\n",
            "    })\n",
            "})\n",
            "从前，有一个小女孩名叫莉莉。她喜欢和家人一起去度假。有一天，她的家人决定去海边旅行。莉莉非常兴奋，她跳起来又跳下去，像发了疯一样。\n",
            "\n",
            "当他们到达海滩时，他们搭起了遮阳伞和毯子。莉莉想立刻去游泳，但她的父母告诉她要等吃完午餐再说。莉莉感到很不耐烦，她说：“我现在就想去游泳！”她妈妈回答：“莉莉，我们需要先吃东西。游泳需要能量。”\n",
            "\n",
            "莉莉意识到妈妈说得对，于是耐心地等待午餐结束。她学会了有时候要克制自己的激动情绪，并听从父母的意见。从那天起，莉莉变得更善于倾听，也更加享受她的假期时光。\n"
          ]
        }
      ],
      "source": [
        "chinese_dataset = load_dataset('json', data_files={'train': TRAIN_FILE, 'validation':VALIDATION_FILE, 'test': TEST_FILE})\n",
        "print(chinese_dataset)\n",
        "print(chinese_dataset[\"test\"][2][\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAcSghJgkUqT"
      },
      "source": [
        "We tokenize the raw text using Llama-2's tokenizer and group the tokenized text as inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUTCHIqEWJRL"
      },
      "outputs": [],
      "source": [
        "block_size = 512\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"])\n",
        "\n",
        "def group_texts(examples):\n",
        "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    if total_length >= block_size:\n",
        "        total_length = (total_length // block_size) * block_size\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXoXO29hWY-v"
      },
      "outputs": [],
      "source": [
        "tokenized_zh_datasets = chinese_dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])\n",
        "lm_datasets = tokenized_zh_datasets.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    batch_size=512,\n",
        "    num_proc=4,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_m505-vnSBg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DUTC5J7nSZt"
      },
      "source": [
        "#### 💡Tutorials: TrainingArguments().\n",
        "\n",
        "**Important Training Hyper-parameters**\n",
        "- learning_rate: The initial learning rate for optimizer.\n",
        "- num_train_epochs: Total number of training epochs to perform (if not an integer, will perform the decimal part percents of the last epoch before stopping training).\n",
        "- *_strategy: The evaluation/saving strategy to adopt during training. Possible values are:\n",
        "    - `\"no\"`: No evaluation/saving is done during training.\n",
        "    - `\"steps\"`: Evaluation/saving is done (and logged) every `eval_steps`.\n",
        "    - `\"epoch\"`: Evaluation/saving is done at the end of each epoch.\n",
        "- per_device_train_batch_size: The batch size per GPU/XPU/TPU/MPS/NPU core/CPU for training.\n",
        "- per_device_eval_batch_size: The batch size per GPU/XPU/TPU/MPS/NPU core/CPU for evaluation.\n",
        "- save_total_limit: If a value is passed, will limit the total amount of checkpoints.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "If you do not understand `AdamW` optimizer and learning scheduler, you may use default settings.\n",
        "\n",
        "**Optimizer Hyper-parameters**\n",
        "- weight_decay: The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in [`AdamW`] optimizer.\n",
        "- adam_beta1: The beta1 hyperparameter for the [`AdamW`] optimizer.\n",
        "- adam_beta2: The beta2 hyperparameter for the [`AdamW`] optimizer.\n",
        "\n",
        "**Learning schedule**\n",
        "- lr_scheduler: The scheduler type to use.\n",
        "- warmup_ratio: Ratio of total training steps used for a linear warmup from 0 to `learning_rate`.\n",
        "\n",
        "[Explore more parameters here](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/trainer#transformers.TrainingArguments)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9OJF3VNhrGK"
      },
      "source": [
        "#### Task 3 Playground\n",
        "\n",
        "---\n",
        "\n",
        "📚 Please just run the following code to do continual pre-training. Please try your best to tune the hyperparameters or collect more data to improve model performance.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lf6bS7lnbPMM"
      },
      "outputs": [],
      "source": [
        "# =========Pre-training hyperparameters, please feel free to tune them~=========\n",
        "# =Important=\n",
        "lr = 1e-4\n",
        "epochs = 8\n",
        "save_steps=200\n",
        "strategy=\"steps\"\n",
        "train_bsz = 32 # reduce batch size if you encountered out-of-memory errors.\n",
        "eval_bsz = 16\n",
        "\n",
        "# If you do not understand AdamW optimizer and learning scheduler, you may use default settings.\n",
        "# =Optimizer=\n",
        "optimizer = \"adamw_torch\"\n",
        "weight_decay = 0.01\n",
        "adam_beta1 = 0.9\n",
        "adam_beta2 = 0.98\n",
        "# =Learning scheduler=\n",
        "lr_scheduler = \"linear\"\n",
        "warmup_ratio = 0.01\n",
        "# =========End of pre-training hyperparameters=========\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    \"llama-42m-zh-fairytales\",\n",
        "    evaluation_strategy = strategy,\n",
        "    eval_steps=save_steps,\n",
        "    save_strategy = strategy,\n",
        "    save_steps=save_steps,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps = 10,\n",
        "    learning_rate=lr,\n",
        "    weight_decay=weight_decay,\n",
        "    seed=42,\n",
        "    per_device_train_batch_size=train_bsz,\n",
        "    per_device_eval_batch_size=eval_bsz,\n",
        "    save_total_limit=1,\n",
        "    optim = optimizer,\n",
        "    lr_scheduler_type = lr_scheduler,\n",
        "    adam_beta1 = adam_beta1,\n",
        "    adam_beta2 = adam_beta2,\n",
        "    warmup_ratio = warmup_ratio,\n",
        "    num_train_epochs = epochs,\n",
        "    report_to=None\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=lm_datasets[\"train\"],\n",
        "    eval_dataset=lm_datasets[\"validation\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "KCIR43fZcAa9",
        "outputId": "adecb9fd-1375-42d9-f7b2-60b91e4dc1de"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2000/2000 12:13, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>3.298300</td>\n",
              "      <td>3.243898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.910000</td>\n",
              "      <td>1.898938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.548700</td>\n",
              "      <td>1.571377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.381600</td>\n",
              "      <td>1.436444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.313700</td>\n",
              "      <td>1.372024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>1.252300</td>\n",
              "      <td>1.322885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>1.179800</td>\n",
              "      <td>1.296618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>1.129800</td>\n",
              "      <td>1.278112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>1.088200</td>\n",
              "      <td>1.270051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>1.106700</td>\n",
              "      <td>1.262663</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2000, training_loss=1.7592032198905945, metrics={'train_runtime': 733.814, 'train_samples_per_second': 86.878, 'train_steps_per_second': 2.725, 'total_flos': 4956004181606400.0, 'train_loss': 1.7592032198905945, 'epoch': 8.0})"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjA9aBL4kUqV"
      },
      "source": [
        "Load pre-trained model and try to generate mini-story in another language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lE_QVLCOkUqV",
        "outputId": "cf3ddb1a-5e89-473d-eeb8-7e32d87b7cb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device type: cuda\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"Device type: {device}\")\n",
        "\n",
        "new_model_path = \"llama-42m-zh-fairytales/checkpoint-2000\" # saved checkpoint path\n",
        "model = LlamaForCausalLM.from_pretrained(new_model_path).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, device=device)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlX2XilNkUqV"
      },
      "source": [
        "Evaluate the PPL on Chinese text (or another language) again.\n",
        "\n",
        "You will notice that we actually achieve a much lower PPL after continual pre-training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "0Ob_g328cGq5",
        "outputId": "e7f2b619-f3c8-4dc2-bbc2-76fac657b2b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Perplexity: 3.53\n",
            "Test Perplexity: 3.52\n"
          ]
        }
      ],
      "source": [
        "data_file = TEST_FILE\n",
        "test_dataset = load_dataset('json', data_files={'test': data_file})[\"test\"][\"text\"]\n",
        "\n",
        "results = compute_ppl(model=model, tokenizer=tokenizer, device=device, inputs=test_dataset, batch_size = 16)\n",
        "dataset_ppl = results['mean_perplexity']\n",
        "print(f\"Test Perplexity: {dataset_ppl:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xgi1EMDycZUp"
      },
      "source": [
        "---\n",
        "\n",
        "The original English base model was pre-trained on 2 million data samples. Considering we are using only 10,000 training samples (0.5% of the original pre-training data), the model can generate a few fluent sentences but may still struggle with long-text generation or common sense of other languages. You can try using more data or training steps depending on your computational resources.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rf9ALF4CEX82",
        "outputId": "66d35223-1871-4d92-a008-cd50f257db4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s> 从前，有一只叫做汤姆的猫。汤姆和他的朋友们一起玩。他们喜欢在公园里玩耍。有一天，汤姆和他的朋友们决定去公园玩。\n",
            "\n",
            "在公园里，汤姆看到了一个大滑梯。他想玩滑梯。他跑去滑梯，但不小心掉在了地上。汤姆很伤心。\n",
            "\n",
            "汤姆很高兴他的朋友们能帮助他。他们一起玩滑梯，度过了很多快乐时光。\n"
          ]
        }
      ],
      "source": [
        "prompt = \"从前，有一只叫做汤姆的猫。汤姆和他的朋友们一起玩。\"\n",
        "\n",
        "# Decoding hyperparameters\n",
        "max_new_tokens = 300\n",
        "do_sample = True\n",
        "temperature = 0.3\n",
        "\n",
        "tokenized_input = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "output_ids = model.generate(\n",
        "    tokenized_input,\n",
        "    max_new_tokens=max_new_tokens,\n",
        "    eos_token_id=1,\n",
        "    do_sample=do_sample,\n",
        "    temperature=temperature,\n",
        ")\n",
        "output_text = tokenizer.decode(output_ids[0])\n",
        "print(output_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJYZCXX3kUqZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "037842bf542143038269aedb14c51c0d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19620102f85d4c6a8b48e1078a180e95": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a50911f63024bf59da9873dd6d1a912": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e9b6ee204f4a457ab9863c0f3baf4864",
              "IPY_MODEL_40a0c17e7fde4bf2862ecda678d5e963",
              "IPY_MODEL_e0cdab58ab0c4936878db9d524c8726c"
            ],
            "layout": "IPY_MODEL_b2143d465f7047629cdfe9777e3a286b"
          }
        },
        "2ae1f05679b84ac9bac39c77485e6b01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2c286ad6343a475d8516ac4779b208ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa04910012354222b3f1a49ed5e33203",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b45c1921cebe42418c9bac889be111de",
            "value": 1
          }
        },
        "2df92583f44640f38112db08ea30cc5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9ef56642df394267850e820ae578d3d7",
              "IPY_MODEL_fb11f7b6a113420cbf9353c79f434ad5",
              "IPY_MODEL_b039f8470ffa4535ad065fc0eda886fd"
            ],
            "layout": "IPY_MODEL_d2f6deec117a4358ad064db89f727bc1"
          }
        },
        "325cd0c8a932466d9c115ffba2904bba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35d813e7b32f44e49d38155e4f950c86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4245c28c8eec4edd921b1755ad1ad45b",
              "IPY_MODEL_2c286ad6343a475d8516ac4779b208ac",
              "IPY_MODEL_62beee732f344d1fabf7e213930ad775"
            ],
            "layout": "IPY_MODEL_037842bf542143038269aedb14c51c0d"
          }
        },
        "35fe5e0e1ce14594b643f9679ea2b7f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "40a0c17e7fde4bf2862ecda678d5e963": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76f64cb2e08e453d977b20150745fe41",
            "max": 63,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_35fe5e0e1ce14594b643f9679ea2b7f3",
            "value": 63
          }
        },
        "4245c28c8eec4edd921b1755ad1ad45b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f004def454c46f4a489d83a265577f6",
            "placeholder": "​",
            "style": "IPY_MODEL_5407caa8f2df48179033c1d0d6daebfe",
            "value": "100%"
          }
        },
        "459a84f78164433884ba5f470a9e1417": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5407caa8f2df48179033c1d0d6daebfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62beee732f344d1fabf7e213930ad775": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_645cdc1efc3344dc8625f9ed98ee4433",
            "placeholder": "​",
            "style": "IPY_MODEL_b2f56d482fdd43dea48c9f649680e11b",
            "value": " 1/1 [00:00&lt;00:00, 19.33it/s]"
          }
        },
        "645cdc1efc3344dc8625f9ed98ee4433": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67c58390dc914bb6a8db182ab1943157": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76f64cb2e08e453d977b20150745fe41": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f004def454c46f4a489d83a265577f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8995660b5da643a495cc5d8d23104913": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91f68453eca74af8804b1871a20a0b26": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ef56642df394267850e820ae578d3d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_459a84f78164433884ba5f470a9e1417",
            "placeholder": "​",
            "style": "IPY_MODEL_19620102f85d4c6a8b48e1078a180e95",
            "value": "100%"
          }
        },
        "b039f8470ffa4535ad065fc0eda886fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e063f6ac5f4a44b88271e700e985185a",
            "placeholder": "​",
            "style": "IPY_MODEL_325cd0c8a932466d9c115ffba2904bba",
            "value": " 63/63 [00:48&lt;00:00,  1.54it/s]"
          }
        },
        "b2143d465f7047629cdfe9777e3a286b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2f56d482fdd43dea48c9f649680e11b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b45c1921cebe42418c9bac889be111de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b5769bc0ec3e4662b08fc27d8dda40c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2f6deec117a4358ad064db89f727bc1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e063f6ac5f4a44b88271e700e985185a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0cdab58ab0c4936878db9d524c8726c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5769bc0ec3e4662b08fc27d8dda40c8",
            "placeholder": "​",
            "style": "IPY_MODEL_91f68453eca74af8804b1871a20a0b26",
            "value": " 63/63 [01:18&lt;00:00,  1.07s/it]"
          }
        },
        "e56cd9f3c4124896a1f04d55b19888ea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9b6ee204f4a457ab9863c0f3baf4864": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e56cd9f3c4124896a1f04d55b19888ea",
            "placeholder": "​",
            "style": "IPY_MODEL_67c58390dc914bb6a8db182ab1943157",
            "value": "100%"
          }
        },
        "fa04910012354222b3f1a49ed5e33203": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb11f7b6a113420cbf9353c79f434ad5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8995660b5da643a495cc5d8d23104913",
            "max": 63,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2ae1f05679b84ac9bac39c77485e6b01",
            "value": 63
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
